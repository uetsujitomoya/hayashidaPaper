
 \begin{figure}
   \begin{center}
     \includegraphics[width=\linewidth]{AutomaticCategoryClassificationFlowchartDiagram.png}
   \end{center}
   \caption{自動カテゴリ分類のフローチャート図}
   \label{fig:3_1}
 \end{figure}
 
  \begin{figure}
   \begin{center}
     \includegraphics[width=\linewidth]{BunNoVectorkaNoNagare.png}
   \end{center}
   \caption{文のベクトル化の流れ}
   \label{fig:3_2}
 \end{figure}


本章では，クライエントの発言内容のカテゴリ分類の手法について詳しく述べる．なお本研究では，会話の流れの可視化システムと同様にクライエントの発言内容を1文毎に分類している．クライエントの1文とは，句点かクエスチョンマークで区切られた単位のことである．また，クライエントの発言内容は上でも述べたように「愛」「交友」「仕事」という3つのカテゴリに分類する．

\section{分類手法の概要}

本研究における分類手法の概要を図3.1に示す．まず，Yahoo!知恵袋\cite{yahoo}の悩み相談に関する文章をコーパスとして，word2vecを用いて，単語のベクトルを得る．word2vecにより得た単語のベクトルを基に，3つのどのカテゴリに属しているかのラベルを持ったYahoo!知恵袋の悩み相談に関する文のベクトルを教師付きデータとする．同様の手順で，クライエントの発言内容を書き起こしたテキストデータを1文毎にベクトル化し，これに正解カテゴリを付与したものをテストデータとする．教師付きデータを機械学習の入力データとし，学習させて，テストデータを学習したモデルに入力し，出力された予測カテゴリとユーザである熟練カウンセラーによる手動修正後カテゴリと一致するかについて評価を行う．

\section{word2vecを用いた単語のベクトル化}

本研究で扱うデータは様々な単語から構成される文章の集まった文書データである．しかしコンピュータで単語や文を扱うために，単語や文を数値ベクトルとして表現する必要がある．機械学習でテキストデータの自動分類を行う研究はこれまでに様々行われてきたが，多くの機械学習のアルゴリズムでは入力の次元数を学習前に定める必要がある．しかし，扱うテキストデータは単語の数，文や文章の長さがそれぞれ異なるものが大半であり，入力するテキストデータを固定長のベクトルで表現する必要がある．このような課題に対して，これまでに主に用いられている手法はbag-of-wordsである．bag-of-wordsは文章を単語の集合として捉え，単語の並び方などは考慮せずに，単語の出現の有無と出現回数のみを考慮する手法である．しかしこの手法を用いると，学習に用いるテキストデータに含まれる語彙数だけベクトルの次元数が増え，語彙数と次元数が等しくなる．そのため，ニューラルネットワークなどのネットワークを用いた機械学習での入力次元が数万〜数十万程度の次元になり，学習を行う際に計算時間が膨大になる．このような次元数の大きいベクトルに対して特異値分解による次元圧縮を行い，それを用いた分類\cite{ShortText}も行われているが，数千次元まで圧縮すると分類精度が下がることが報告されている．それに対して，単語の分散表現を学習することで単語を数百次元程度の低次元のベクトルで表現する手法であるword2vecがMikolovら\cite{pylearn}により提案された．bag-of-wordsのようにベクトルの1つの次元のみが値を持ち，その他の次元の値が0となる疎なベクトル表現とは違い，分散表現では全ての次元が値を持っている．本研究で機械学習を行う際に扱うテキストデータには数万以上の語彙数が含まれているため，word2vecを用いて単語のベクトル化を行うこととする．word2vecに学習させるコーパスとして，Wikipediaやニュース記事のような大規模データが用いられることが多い．そこから得られた単語の分散表現を用いて機械学習などを用いて分類を行う研究では，分類の対象がニュース記事などであることが多いためである．しかし本研究で扱うテキストデータはカウンセリングデータで会話文である．Wikipediaやニュース記事は，固有名詞が比較的多く出現するが，個人の感情を表す形容詞はあまり含まれていない．しかしカウンセリングデータは会話文であるため固有名詞は少なく，感情を表す形容詞は比較的多く含まれる．しかし，カウンセリングの内容を書き起こしたテキストデータはあまり多く存在しておらず，且つプライベートな内容であるためインターネットなどでは公開されておらず，収集するのが困難である．そこで，カウンセリングデータとより似た内容の文章をコーパスに用いることを考えた．そのためカウンセリングデータに比較的内容が近いと思われるYahoo!知恵袋に投稿されたものの中で悩み相談に関するカテゴリから文章を取得した．具体的には，本研究で分類を行う「愛」「交友」「仕事」の3カテゴリと内容が類似していると思われる，「恋愛相談」「家族関係の悩み」「友人関係の悩み」「職場の悩み」の4カテゴリから取得した． 4カテゴリの中から，1つの質問とそれに対してのベストアンサーを1件として，各カテゴリ12000件，計48000件を取得してコーパスとした．また，word2vecは元々英語の文章を対象に考えられているため，コーパスは単語同士が半角スペースで区切られている必要がある．そのため，本研究では形態素解析ツールMeCab\cite{Mecab}を用いて予め知恵袋の文章を単語毎に区切った文章を入力した．また，コーパスに対して形態素解析を行う際に，全ての単語を基本形で出力している．

次にword2vecで学習を行う際のパラメータについて述べる．ベクトルの次元数に関して，word2vecの開発者であるMikolovらによると，コーパスのデータサイズが増えるにつれてベクトルの次元数も大きくすべきとされている\cite{linguistic-regularities-in-continuous-space-word-representations}．また，データサイズが大きくないにも関わらずベクトルの次元数を大きくしすぎた場合，精度が落ちることも報告されている．word2vecのデフォルトの次元数は100次元であり，本研究で用いたコーパスはデータサイズが約40MB，語彙数がおよそ1万語と大きくないため，ベクトルの次元数は200以下とした．窓長は各窓長に対して学習後の各単語の意味的な類似度を調べた際に適切な値とされる5に設定した．学習アルゴリズムとしてcbowモデルもしくはSkip-gramモデルを選択できるが，本研究ではSkip-gramモデルを選択した．また，min-countの値は5とし，コーパス内に5回未満しか出現しない単語は考慮しないとした．このような手順で単語の分散表現が得られる．
\section{文のベクトル化} 
3.2節で述べた単語の分散表現を用いて，文をベクトルに変換する．word2vecにより単語の分散表現を得た際と同様にして，まず文を単語毎に区切る必要がある．次に，文中に出現する各単語のベクトルをword2vecにより得た分散表現から見つけ出し，それを用いて文ベクトル化を行う．具体例として，「母ともよく喧嘩しますし．」という文ベクトル化の流れを図3.2に示し，以下にその概要を述べる．
\begin{enumerate}
\item 「母ともよく喧嘩しますし．」という文に対して形態素解析により単語毎の分かち書きを行い，基本形で出力する．
\item 文中に出現する全ての単語に対して，各単語のベクトルをword2vecにより学習したモデルから取り出す．ただし文末の句点とクエスチョンマークは除く．
\item 「母」，「とも」，「よく」，「喧嘩」，「する」，「ます」，「し」の単語ベクトルの1次元目の値を全て足し合わせ，各単語のベクトルの1次元目の和を計算する．このとき，単語がword2vecにより学習したモデルに存在せずにベクトルが存在しなかった場合，その単語は無視する．
\item 1次元目に対して行った手順をベクトルの次元全てに対して行い，全ての次元分の和を計算する
\item 各文の長さは異なり，また単語の数も異なるので，足し合わせた単語数で各次元の和を割り，これを文のベクトルとする
\end{enumerate}
各単語と文ベクトルの次元数をMとして，文ベクトルを

\begin{equation}
  d=[d_1, d_2, d_3, …, d_M ]  
\end{equation}

とし，足し合わせた単語の，i番目の単語のj次元目の要素の値を$w_ij$とすると，

\begin{equation}
  d_1=\frac{1}{N}\sum_{i=1}^n w_i1 ,　d_2=\frac{1}{N}\sum_{i=1}^n w_i2 ,　…　,　d_M=\frac{1}{N}\sum_{i=1}^n w_iM
\end{equation}と表せる．Xingら\cite{dcdwv}は，機械学習による文書分類を行う際に，word2vecで得た単語ベクトルの単純な和の平均として文章ベクトルを作成する手法により，分類精度が高くなることを示している．本研究でも，文ベクトルを単語ベクトルの和の平均として算出した．
\section{文ベクトルを入力とする機械学習}
本節では，本研究における機械学習を用いた分類手法について述べる．本研究で行う機械学習は教師付き学習であり，教師付き学習とは，入力のデータに対して出力が指定されて学習を行う方法である．本研究では，3.3節で述べた手順で算出した1文毎の文ベクトルを，入力である1文毎の特徴量として用いて，出力には「愛」「交友」「仕事」の3つの分類カテゴリを用いる．学習を行ったパターン認識器に対して，どのカテゴリに属するか未知であるクライエントの発言1文毎の文ベクトルを入力し，予測カテゴリを出力させることで，自動カテゴリ分類を行う．本研究では，機械学習に用いるアルゴリズムとして，SVM\cite{paturn}とニューラルネットワーク\cite{newralnetwork}を用いた．
SVMはパターン認識モデルの1つであり，線形しきい素子を用いて2つのカテゴリのパターン識別機を構成する手法である．2つのカテゴリに分類を行う際に，ハードマージンSVMでは入力データが完全に線形分離可能であると仮定して，1つの誤分類も許容せずに分離超平面を決定する．つまり，M個のm次元教師付きデータ$x_i$(i=1， 2，…， M)がクラス1， 2いずれかに属するとして，ラベルをクラス1のときに$y_i$=1， クラス2のときに-1とする．線形分離可能である場合，決定関数はwをm次元係数ベクトル，bはバイアス項として
\begin{equation}
  D(x)=w^T x+b 
\end{equation}                      % (3)
と決めることが出来る．また線形分離の条件から決定関数は
\begin{equation}
  y_i (w^T x_i+b)\geq1      (i=1,…,M)   
\end{equation}                     %       (4)
の条件を満たす．条件式(3.4)を満たすようなw，bを求め，入力データに対して分離超平面を決定するが，このとき分離超平面とそれにもっとも近い教師データとの間の距離をマージンとよぶ．条件式(3.4)を満たす分離超平面は無数に存在するが，SVMではマージンが最大となる超平面を識別境界とする．したがって，SVMでは汎化能力が高いというのが大きな特徴である．
しかし，本研究では入力データは完全に線形分離可能ではないと考え，ソフトマージンSVMを用いた．ソフトマージンSVMでは線形分離可能でない場合に適用できるように，$\xi_i (\geq0)$を導入して条件式(3.4)を拡張し，
\begin{equation}
  y_i (w^T x_i+b)\geq1-\xi_i      (i=1,…,M)   
\end{equation}     %                (5)
を満たし，
\begin{equation}
  Q(w,b,\xi)=\frac{1}{2} ‖w‖^2+C\sum_{i=1}^M \xi_i 
\end{equation} %　　　　             (6)
を最小にするような最適化問題を考え分離超平面を決定する．したがってハードマージンSVMとは異なり，ソフトマージンSVMでは誤分類を許容する．ここでCは式(3.6)の右辺第1項のマージン最大化と第2項の誤分類の最小化のトレードオフを決定するパラメータであり，分離超平面の決定に大きな影響を与えるため，適切に設定する必要がある．
そして，このように入力空間内で非線形分離を行う必要がある場合には入力空間を高次元の特徴空間に写像して特徴空間上でマージンが最大となるように超平面を決定するカーネル法とよばれている方式がよく用いられており，本研究でもこの方式を用いた．カーネル法では，非負のラグランジュ乗数$\alpha_i，\beta_i$を導入して，決定関数は
\begin{equation}
  D(x)=\sum_{i \in S}^{} \alpha_i y_i K(x_i,x_j)+b
\end{equation}%                         (7)
となる．ここで$K(x_i,x_j)$はカーネルであり，問題に応じてこのカーネルを適切に選ぶことで汎化能力を向上することができる．本研究では，
\begin{equation}
  K(x,x')=x^T x' 
\end{equation}%                        (8)
で表される線形カーネル(linear)と，
\begin{equation}
  K(x,x')=exp(-\gamma‖x-x' ‖^2 )
\end{equation}
で表されるラジアル基底関数カーネル(rbf)を用いた．ここでγは分布の半径を制御するパラメータであり，rbfカーネルにおいて超平面の決定に大きな影響を与える．
また，上でも述べた通りSVMは2つのカテゴリに分類する2値分類を基本としているが，本研究では3つのカテゴリに分類することを目的としている．SVMにおいて多カテゴリへの分類を行う場合，One-Vs-One方式とOne-Vs-The-Rest方式のいずれかを用いる．今，Kカテゴリに分類することを考える．One-Vs-One方式はある特定のカテゴリに属するか，また別の特定のカテゴリに属するかの2カテゴリ分類問題を解く分類器を，全てのカテゴリの組み合わせ，つまりK(K-1)/2個使用する．One-Vs-The-Rest方式はある特定のカテゴリに属するか，他のK-1個のいずれかのカテゴリに属するかの2カテゴリ分類問題を解く分類器を，全ての組み合わせ，K個利用する．

ニューラルネットワークもパターン認識に用いられるモデルの1つである．本研究で用いたニューラルネットワークは入力層と出力層を1つずつ持ち，中間層を複数持つ多層ニューラルネットワークである．また全てのノード同士がリンクによって繋がれた，全連結型のネットワークである．ニューラルネットワークの仕組みの概要として，入力層の各ノードに値を入力し，それぞれの値が重みを持ったノード間のリンクを通って，閾値を持った中間層の各ノードに受け渡され，その値が中間層で変換される．中間層で変換される際には活性化関数を用いて変換が行われる．そして変換された値が同じように次の中間層へと受け渡されていき，最後に出力層に伝達し値を出力させる．そして出力した値が期待される値に近づくように，各ノード間のリンクの重みやノードの閾値を最適化するように学習を行う．
本研究では入力層のノード数を文ベクトルの次元数と等しく固定し，ベクトルの各要素の値を入力層の各ノードに入力する．出力層のノード数は分類カテゴリ数と同じ3で固定し，出力が最も大きいカテゴリをその文が属するカテゴリであると判断する．また中間層の層数，ノード数は可変なパラメータとしており，このパラメータによって精度が変わるため，SVMと同様に後述の実験により最適なパラメータを求める．
本研究での分類の手順は，初めに3.3節で作成した教師データを上で述べた機械学習の入力として学習を行う．そして学習後のモデルにテストデータを入力し，「愛」「交友」「仕事」のいずれかの予測カテゴリを出力させ3カテゴリへの分類を行う．